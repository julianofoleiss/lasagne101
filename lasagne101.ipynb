{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Um Tutorial Rápido sobre *Deep Learning* e Lasagne\n",
    "Juliano Henrique Foleiss\n",
    "\n",
    "## O que é *deep learning*?\n",
    "\n",
    "*Deep learning* (aprendizagem profunda)é um paradigma de aprendizagem de máquina onde as características são aprendidas a partir de representações alternativas dos dados de entrada. Usualmente, estas características são arranjadas de forma hierárquica, de forma que as características mais internas representam agregações e combinações de características mais externas. Isto contrasta com a abordagem tradicional *shallow learning* (aprendizagem rasa), onde o as características são determinadas com base em conhecimento especialista no domínio do problema. Neste caso, o aprendizado de máquina serve como mecanismo de separação ou combinação destas características para classificação ou regressão, sem a necessidade de derivar novas características.\n",
    "\n",
    "Os modelos de *deep learning* consistem em múltiplas camadas de operações neurais, onde a saída de uma camada é entrada para a próxima. Usualmente, estas redes podem ser treinadas usando *backpropagation*, desde que as operações realizadas em cada camada sejam diferenciáveis. Também existem redes profundas com camadas recorrentes. Nestes casos, BTT (backpropagation through time) pode ser utilizado como procedimento de treino.\n",
    "\n",
    "## O que é *lasagne*?\n",
    "\n",
    "*Lasagne* é uma biblioteca escrita em *Python* que permite a prototipação rápida de redes neurais profundas. Por ser implementada sobre uma biblioteca de computação tensorial com capacidade de geração de código pra GPUs (*theano*), Lasagne pode ser usada para treinar modelos suficientemente grandes com eficiência. Mesmo sem GPU é possível desenvolver código que rode em máquinas equipadas com GPU, uma vez que o código que roda em ambas situações é exatamente o mesmo.\n",
    "\n",
    "## Que *software* vou precisar neste tutorial?\n",
    "\n",
    "  * Python 2.7\n",
    "  * Compilador C\n",
    "  * BLAS (biblioteca C/Fortran) -- **libopenblas-dev** (ubuntu / debian)\n",
    "  * numpy\n",
    "  * scipy\n",
    "  * matplotlib\n",
    "  * sklearn\n",
    "  * lasagne (http://lasagne.readthedocs.io/en/latest/user/installation.html)\n",
    "  * theano\n",
    "  \n",
    "## Dicas de instalação\n",
    "\n",
    "  * Se você usa Linux, o Python 2.7 já está instalado no seu computador. Com certeza.\n",
    "  * O compilador C mais comum é o GCC. O pacote no ubuntu é **gcc**.\n",
    "  * Bibliotecas numpy e scipy são bibliotecas do Python. Recomendo usar a ferramenta pip para instalar estes pacotes, uma vez que são versões mais atualizadas que do seu gerenciador de pacotes. Basta usar\n",
    "  \n",
    "```\n",
    "[sudo] pip install [--user] numpy scipy matplotlib sklearn\n",
    "```\n",
    "  Use **sudo** pra instalar para todos os usuários. Caso for instalar só para o seu usuário, use **-user**.\n",
    "  * Use o link (http://lasagne.readthedocs.io/en/latest/user/installation.html) para instalar o lasagne. Este link contém instruções como instalar o theano também. Use as instruções da Seção (**Bleeding-edge version**)\n",
    "  \n",
    "## Sobre os exemplos de código abaixo\n",
    "\n",
    "Os exemplos de código abaixo são adaptações do tutorial do lasagne, disponível em http://lasagne.readthedocs.io/en/latest/user/tutorial.html\n",
    "\n",
    "Todas as explicações antes dos trechos de código e nos comentários são puramente intuições. Existem explicações muito bem elaboradas e matematicamente completas para os conceitos. No entanto, com o intuito de manter a discussão acessível e com caráter prático, resolvi omiti-las. Encorajo fortemente aos colegas que estudem estes conceitos de forma completa antes de usar as ferramentas explicadas neste tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema: Reconhecimento de Dígitos Manuscritos\n",
    "\n",
    "O dataset MNIST (LeCun et. al., 1998) consiste em 60000 dígitos manuscritos centralizados em imagens monocromáticas 28x28 para treino e 10000 exemplos para teste. As classes consistem nos 10 dígitos do sistema decimal (0-9). A base está hospedada no site do Yann LeCun, e o código abaixo pode ser usado para obtê-la facilmente a partir da internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ################## Download and prepare the MNIST dataset ##################\n",
    "# This is just some way of getting the MNIST dataset from an online location\n",
    "# and loading it into numpy arrays. It doesn't involve Lasagne at all.\n",
    "\n",
    "def load_dataset():\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rotina abaixo mostra como montar uma rede neural usando lasagne. Note que o nome da biblioteca é bem sugestivo: a idéia é empilhar camadas de redes neurais como empilhamos deliciosas camadas de massa, queijo, molhos e outras delícias para fazer nosso prato favorito.\n",
    "\n",
    "Note também que a função abaixo apenas define a arquitetura da rede. Esta função não define o algoritmo de otimização que vamos usar no treino, nem define uma interface de como iremos invocar as operações desta rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def montar_mlp(var_entradas=None):\n",
    "    # Esta função monta uma MLP com duas camadas ocultas com 800 neuronios cada, \n",
    "    # seguida por uma camada softmax com 10 neuronios. Na entrada é aplicado dropout de 20%\n",
    "    # e nas camadas ocultas é aplicado dropout de 50%. (https://arxiv.org/pdf/1207.0580.pdf pra entender dropout)\n",
    "\n",
    "    l_ent = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                      input_var=var_entradas)\n",
    "    \n",
    "    l_ent_drop = lasagne.layers.DropoutLayer(l_ent, p=0.2)\n",
    "    \n",
    "    l_oc1 = lasagne.layers.DenseLayer(l_ent_drop,\n",
    "                                      num_units=800, \n",
    "                                      nonlinearity=lasagne.nonlinearities.rectify, \n",
    "                                      W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    l_oc1_drop = lasagne.layers.DropoutLayer(l_oc1, p = 0.5)\n",
    "    \n",
    "    l_oc2 = lasagne.layers.DenseLayer(l_oc1_drop,\n",
    "                                      num_units=800, \n",
    "                                      nonlinearity=lasagne.nonlinearities.rectify, \n",
    "                                      W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    l_oc2_drop = lasagne.layers.DropoutLayer(l_oc1, p = 0.5)\n",
    "    \n",
    "    l_saida = lasagne.layers.DenseLayer(l_oc2_drop, \n",
    "                                      num_units=10, \n",
    "                                      nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_saida\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com deep learning é comum que as bases de dados sejam grandes. Isto vem do fato que, como as features são aprendidas a partir dos dados, precisamos de muitos exemplos para que não haja *overfitting* apenas nos casos mais comuns. Além do mais, como a quantidade de parâmetros do modelo costuma ser muito grande, muitos exemplos são necessários para representar o espaço de busca de forma satisfatória (https://en.wikipedia.org/wiki/Curse_of_dimensionality).\n",
    "\n",
    "Desta forma, usualmente não é possível apresentar todos os exemplos de uma só vez para a rede. Também não é eficiente apresentar apenas um por vez. Uma estratégia comum é treinar a rede em mini-batches. A função **iterate_minibatches** abaixo é uma corotina (*python generator function*) que retorna subconjuntos de tamanho *batchsize* correspondentes na primeira dimensão em inputs (X_*) e targets (Y_*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_EPOCAS = 30\n",
    "TAM_BATCH = 500\n",
    "\n",
    "print (\"Carregando dataset...\")\n",
    "#Carregar o dataset MNIST\n",
    "#Os tensores de exemplos X_* possuem as dimensões (N_EXEMPLOS, CANAIS_DE_COR, LINHAS, COLUNAS).\n",
    "X_treino, Y_treino, X_teste, Y_teste = load_dataset()\n",
    "\n",
    "#Dividir o conjunto de treino em treino e validação.\n",
    "X_treino, X_val, Y_treino, Y_val = train_test_split(X_treino, Y_treino, test_size=0.2)\n",
    "\n",
    "#Declarar as variáveis Theano para as entradas e saídas\n",
    "var_entradas = T.tensor4()\n",
    "var_saidas = T.ivector()\n",
    "\n",
    "print (\"Montando rede...\")\n",
    "rede = montar_mlp(var_entradas=var_entradas)\n",
    "\n",
    "#Para realizar o treinamento da rede, é necessário computar uma função de custo. Neste caso,\n",
    "#vamos usar a média função de entropia cruzada entre as predições da rede e os valores corretos.\n",
    "\n",
    "#get_output retorna uma expressão theano que representa a operação \"forward\" da rede.\n",
    "#Veja que ainda não estamos fazendo as predições! Estamos apenas montando a expressão de custo!\n",
    "predicao = lasagne.layers.get_output(rede)\n",
    "custo = lasagne.objectives.categorical_crossentropy(predicao, var_saidas)\n",
    "custo = custo.mean()\n",
    "\n",
    "#Com a função de custo em mãos, é possível definir a estratégia de atualização dos pesos da rede durante o treino.\n",
    "#Abaixo usamos gradiente estocástico descendente com momento Nesterov.\n",
    "\n",
    "#get_all_params é usada para pegar uma lista de todas as variáveis theano do modelo. trainable=True\n",
    "#indica que queremos parametros apenas das camadas com parametros treináveis (Dropout não tem, por exemplo).\n",
    "params = lasagne.layers.get_all_params(rede, trainable=True)\n",
    "\n",
    "#neste caso, atualizacao é uma expressão theano que faz um passo e atualização nos parametros.\n",
    "atualizacao = lasagne.updates.nesterov_momentum(custo, \n",
    "                                                params, \n",
    "                                                learning_rate=0.01, \n",
    "                                                momentum=0.9)\n",
    "\n",
    "#Agora precisamos de expressões para a predição no teste, o cálculo do custo no teste e uma expressão para calculo\n",
    "#da acurácia (como brinde!)\n",
    "\n",
    "#A flag deterministic=True indica que o comportamento desta expressão deve ser determinístico.\n",
    "#(Com efeito, esta flag desabilita as camadas de dropout)\n",
    "teste_predicao = lasagne.layers.get_output(rede, deterministic=True)\n",
    "teste_custo = lasagne.objectives.categorical_crossentropy(teste_predicao, var_saidas)\n",
    "teste_custo = teste_custo.mean()\n",
    "\n",
    "teste_ac = T.mean( T.eq( T.argmax( teste_predicao, axis=1 ), var_saidas ), dtype=theano.config.floatX )\n",
    "\n",
    "print (\"Compilando rotinas...\")\n",
    "\n",
    "#Agora sim, vamos compilar as rotinas de treino e teste que podem ser chamadas do Python! Note que tudo até agora\n",
    "#é preparação para montar a rede e especificar seus parametros de treinamento.\n",
    "\n",
    "fn_treino = theano.function([var_entradas, var_saidas], custo, updates=atualizacao)\n",
    "\n",
    "fn_val = theano.function( [var_entradas, var_saidas], [teste_custo, teste_ac] )\n",
    "\n",
    "custos_treino = []\n",
    "custos_val = []\n",
    "acs_val = []\n",
    "\n",
    "print(\"Treinando modelo...\")\n",
    "t_ini = time.time()\n",
    "for e in xrange(N_EPOCAS):\n",
    "    custo_treino = 0\n",
    "    bat_treino = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(X_treino, Y_treino, TAM_BATCH):\n",
    "        entradas, gabaritos = batch\n",
    "        custo_treino += fn_treino(entradas, gabaritos)\n",
    "        bat_treino += 1\n",
    "    \n",
    "    custos_treino.append( (custo_treino / bat_treino) )\n",
    "    \n",
    "    custo_val = 0\n",
    "    ac_val = 0\n",
    "    bat_val = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(X_val, Y_val, TAM_BATCH):\n",
    "        entradas, gabaritos = batch\n",
    "        err, ac = fn_val(entradas, gabaritos)\n",
    "        custo_val += err\n",
    "        ac_val += ac\n",
    "        bat_val += 1\n",
    "        \n",
    "    custos_val.append((custo_val / bat_val))\n",
    "    acs_val.append((ac_val / bat_val))\n",
    "t_fim = time.time()\n",
    "print(\"O treino durou %.2f segundos! (média de %.2fs por época)\" %  (t_fim - t_ini, (t_fim - t_ini ) / N_EPOCAS ) )\n",
    "    \n",
    "print(\"Testando modelo...\")    \n",
    "\n",
    "custo_teste = 0\n",
    "ac_teste = 0\n",
    "bat_teste = 0\n",
    "\n",
    "for batch in iterate_minibatches(X_teste, Y_teste, TAM_BATCH):\n",
    "    entradas, gabaritos = batch\n",
    "    err, ac = fn_val(entradas, gabaritos)\n",
    "    custo_teste += err\n",
    "    ac_teste += ac\n",
    "    bat_teste += 1\n",
    "\n",
    "print(\"Erro no conjunto de TESTE: %.2f\\nAcurácia no conjunto de TESTE: %.2f\" % ( (custo_teste/bat_teste), (ac_teste/bat_teste)   ) )\n",
    "\n",
    "\n",
    "fig, eixos = plt.subplots()\n",
    "eixos.plot(custos_treino)\n",
    "eixos.plot(custos_val)\n",
    "eixos.set_ylabel(\"Erro\")\n",
    "eixos.set_xlabel(u\"Épocas\")\n",
    "\n",
    "fig, eixos = plt.subplots()\n",
    "eixos.plot(acs_val)\n",
    "eixos.set_ylabel(u\"Acurácia\")\n",
    "eixos.set_xlabel(u\"Épocas\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
